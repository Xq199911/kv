import sys
import os

# Add parent directory to path before importing StreamingLLM_GPE modules
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from transformers import AutoTokenizer, AutoConfig
os.environ["HF_DATASETS_DISABLE_CACHE"] = "1"
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
from accelerate import Accelerator
import peft
from argparse import ArgumentParser
import logging
from transformers import BitsAndBytesConfig

from StreamingLLM_GPE.dataloader_hf import StreamingDataCollator
from StreamingLLM_GPE.models.Gemma2.gemma2_streaming import Gemma2ForCausalLM_stream
from StreamingLLM_GPE.models.Llama3.llama_streaming import LlamaForCausalLM_stream
from StreamingLLM_GPE.models.Qwen2_5.qwen_streaming import Qwen2ForCausalLM_stream
from StreamingLLM_GPE import utils
from StreamingLLM_GPE.evaluate.lagging import calculate_al_and_laal
import sacrebleu


def get_args():
    parser = ArgumentParser()
    parser.add_argument("--pe_cache_length", type=float, default=0)
    parser.add_argument("--inference_mode", type=str, default="streaming",
                        choices=["batch", "streaming"])
    parser.add_argument("--LLM_backbone", type=str, default="Qwen",
                        choices=["Qwen", "Llama", "Gemma"])
    parser.add_argument("--LLM_path", type=str, required=True, help="Path to the LLM model.")
    parser.add_argument("--lora_path", type=str, default=None)
    parser.add_argument("--wait_k", type=int, default=5)
    parser.add_argument("--device", type=int, default=0)
    parser.add_argument("--output_dir", type=str, default='./output')
    parser.add_argument("--split_mode", type=str, default="word",
                        choices=["word", "token"])
    parser.add_argument("--params", type=str, default="./configs/params_qwen_inference.json")
    return parser.parse_args()


def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True


def main():
    args = get_args()
    params = utils.Params(args.params)
    setup_seed(0)
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    logging.basicConfig(
        filename=f"{args.output_dir}/output.log",
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    config = AutoConfig.from_pretrained(args.LLM_path)
    config._attn_implementation = "eager"
    tokenizer = AutoTokenizer.from_pretrained(args.LLM_path, padding_side='right', config=config)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    CausalLM_stream = {
        "Qwen": Qwen2ForCausalLM_stream,
        "Llama": LlamaForCausalLM_stream,
        "Gemma": Gemma2ForCausalLM_stream,
    }
    CausalLM_stream = CausalLM_stream[args.LLM_backbone]

    # 1. 定义量化配置
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4"
    )

    # 2. 加载带量化的模型
    model = CausalLM_stream.from_pretrained(
        args.LLM_path,
        ignore_mismatched_sizes=True,
        config=config,
        quantization_config=quantization_config,
        device_map="auto"  # 让库自动分配显存，防止OOM
    )

    if args.lora_path is not None:
        model = peft.PeftModel.from_pretrained(model, args.lora_path)

    if model.config.pad_token_id is None:
        model.config.pad_token_id = tokenizer.eos_token_id

    # StreamingDataCollator
    data_collator = StreamingDataCollator(
        file_path=params.file_path,
        tokenizer=tokenizer,
        Instruct=params.Instruct,
        user_Instruct=params.user_Instruct,
        assistant_Instruct=params.assistant_Instruct,
        end_Instruct=params.end_Instruct,
        source_key=params.source_key,
        target_key=params.target_key,
        inference_mode=args.inference_mode,  # "batch", "streaming"
        split_mode=args.split_mode,  # "word", "token"
        if_add_space=params.if_add_space,
        pe_cache_length=args.pe_cache_length,
        wait_k=args.wait_k,
    )

    data_collator_dataset = data_collator.dataset_loader()
    dataloader = DataLoader(
        data_collator_dataset,
        batch_size=1,
        shuffle=False,
        collate_fn=data_collator.collate_fn_inference
    )

    accelerator = Accelerator(mixed_precision="bf16")
    stream_model, dataloader = accelerator.prepare(model, dataloader)

    target_txt_lt = []
    output_text_lt = []
    AL = []
    LAAL = []

    stream_model.eval()
    for step, batch in enumerate(tqdm(dataloader)):
        input_ids = batch.get("source_tokens", None)
        attention_mask = batch.get("attention_mask", None)
        _lengths = batch.get("_lengths", None)
        # position_ids = batch.get("position_ids", None)
        # ...
        inference_mode = batch.get("inference_mode", "streaming")
        split_mode = batch.get("split_mode", None)
        _lengths_index = batch.get("_lengths_index", None)
        wait_k = batch.get("wait_k", None)
        assistant_token = batch.get("assistant_token", None)
        source_txt = batch.get("source_txt", None)
        target_txt = batch.get("target_txt", None)

        if inference_mode == "streaming":
            output_sequences, wait_lagging = stream_model.generate(
                input_ids=input_ids.to(accelerator.device),
                attention_mask=attention_mask.to(accelerator.device),
                max_new_tokens=1024,
                generate_mode=inference_mode,
                split_mode=split_mode,
                pe_cache_length=args.pe_cache_length,
                tokenizer=tokenizer,
                end_Instruct=params.end_Instruct,
                _lengths=_lengths,
                _lengths_index=_lengths_index,
                wait_k=wait_k,
                source_words=source_txt,
                assistant_token=assistant_token.to(accelerator.device),
                # # output_attentions = True,
            )
        elif inference_mode == "batch":
            output_sequences, wait_lagging = stream_model.generate(
                input_ids=input_ids.to(accelerator.device),
                attention_mask=attention_mask.to(accelerator.device),
                max_new_tokens=1024,
            )

        output_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)
        target_txt_lt.extend(target_txt)
        output_text_lt.extend([output_text, ])

        if inference_mode == "streaming":
            output_text_len = len(output_text.split())
            al, laal = calculate_al_and_laal(len(_lengths[0]['source_seg_len']) - 1, output_text_len,
                                             wait_lagging[:output_text_len])
            AL.append(al)
            LAAL.append(laal)

        if accelerator.is_main_process:
            print("streaming-input:\n", source_txt)
            print("streaming-output:\n", output_text)

    bleu = sacrebleu.corpus_bleu(output_text_lt, [target_txt_lt])
    logging.info(f"BLEU score: {bleu.score:.2f}")

    if inference_mode == "streaming":
        avg_LAAL = sum(LAAL) / len(LAAL)
        avg_AL = sum(AL) / len(AL)
        logging.info(f"all_AL score: {avg_AL:.2f}")
        logging.info(f"all_LAAL score: {avg_LAAL:.2f}")


if __name__ == "__main__":
    main()
