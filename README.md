# Head-Aware Dynamic KV Budgeting

A-Level Paper Project: Efficient Long-Sequence Inference for Large Language Models

## ğŸ¯ Project Goal

Publish an A-level conference/journal paper (ACL, EMNLP, NeurIPS, ICML, etc.)

**Research Question**: How to efficiently compress KV cache for long-sequence inference by leveraging attention head functionality?

**Core Method**: 
- **Head-Aware Cache**: Dynamic KV cache budget allocation based on attention head functionality
- **Group-Aware Eviction**: Collaborative eviction strategy based on head groups

**Baseline Comparisons**:
- H2O (Heavy-Hitter Oracle)
- StreamingLLM (Fixed Window + Attention Sinks)

---

## ğŸš€ Quick Start

**è¯¦ç»†æ­¥éª¤**: å‚è§ `scripts/ubuntu/README.md`

### Windowsç³»ç»Ÿ

```powershell
# 1. ç¯å¢ƒå‡†å¤‡
python check_environment.py
python -m pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# 2. ä¸‹è½½æ¨¡å‹
.\scripts\windows\download_models.ps1

# 3. è¿è¡Œå®éªŒ                                                                                                                                 
.\scripts\windows\run_a_level_experiments.ps1
```

**è¯¦ç»†æ­¥éª¤**: å‚è§ `scripts/windows/README.md`

---

## ğŸ“š Documentation

1. **`scripts/ubuntu/README.md`** â­ - Ubuntuç³»ç»Ÿå®Œæ•´è¿è¡ŒæŒ‡å—
2. **`scripts/windows/README.md`** â­ - Windowsç³»ç»Ÿå®Œæ•´è¿è¡ŒæŒ‡å—
3. **`EXPERIMENT_GUIDE.md`** - é€šç”¨å®éªŒæŒ‡å—
4. **`A_LEVEL_PAPER_FINAL_GUIDE.md`** - è¯¦ç»†å®éªŒæµç¨‹
5. **`THEORETICAL_ANALYSIS.md`** - ç†è®ºåˆ†æ

---

## ğŸ“ Project Structure

```
StreamingLLM/
â”œâ”€â”€ StreamingLLM_GPE/          # æ ¸å¿ƒä»£ç ï¼ˆè·¨å¹³å°ï¼‰
â”‚   â”œâ”€â”€ baselines/             # Baselineå®ç° (H2O, StreamingLLM)
â”‚   â”œâ”€â”€ models/                # æ¨¡å‹å®ç° (Qwen, Llama, Gemma)
â”‚   â”œâ”€â”€ evaluate/              # è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ utils/                 # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ scripts/                    # ç³»ç»Ÿç‰¹å®šè„šæœ¬
â”‚   â””â”€â”€ windows/               # Windowsç³»ç»Ÿè„šæœ¬
â”‚       â”œâ”€â”€ download_models.ps1
â”‚       â”œâ”€â”€ run_a_level_experiments.ps1
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ models/                     # æ¨¡å‹æ–‡ä»¶
â”œâ”€â”€ data_raw/                   # åŸå§‹æ•°æ®
â”œâ”€â”€ output_logs/                # å®éªŒç»“æœè¾“å‡º
â”‚
â”œâ”€â”€ download_models_china.py     # æ¨¡å‹ä¸‹è½½ï¼ˆPython 3.9+ï¼‰
â”œâ”€â”€ download_models_python38.py  # æ¨¡å‹ä¸‹è½½ï¼ˆPython 3.8ï¼‰
â”‚
â”œâ”€â”€ check_environment.py         # ç¯å¢ƒæ£€æŸ¥
â”œâ”€â”€ check_model_integrity.py     # æ¨¡å‹æ£€æŸ¥
â”œâ”€â”€ test_baselines.py           # Baselineæµ‹è¯•
â”œâ”€â”€ analyze_experiment_results.py # ç»“æœåˆ†æ
â”œâ”€â”€ visualize_results.py        # å¯è§†åŒ–
â”‚
â””â”€â”€ README.md                    # æœ¬æ–‡ä»¶
```

---

## ğŸ“Š Experiment Requirements

### Must Complete (A-Level Paper)

1. âœ… **Baseline Implementation** (H2O, StreamingLLM)
2. âœ… **Long Sequence Memory Efficiency Comparison**
3. âœ… **Ablation Study** (prove component contributions)
4. âœ… **Multi-Model Validation** (at least 1 model, 3 recommended)

---

## ğŸ”¬ Experiments

### Phase 1: Long Sequence Memory Efficiency

- Sequence lengths: 2000, 5000, 10000, 20000 tokens
- Methods: Baseline (GPE), H2O, StreamingLLM, Head-Aware
- Samples: 100 per method

### Phase 2: Budget Impact Analysis

- Budgets: 2048, 4096, 8192 tokens/layer
- Method: Head-Aware

### Phase 3: Ablation Study

- Configurations: Baseline, Head-Aware
- Sequence length: 5000 tokens

### Phase 4: Results Analysis

- Automatic analysis and visualization
- Generate tables and figures for paper

---
